---
# ==============================================================================
# 36_trading_ibkr_airflow_kafka_spark.yml
# Infra PV - Trading Algorítmico: IBKR (master) + Airflow (NOC) + (stubs Kafka/Spark)
# - IBKR en pv-infra-master (VNC 5900)
# - Airflow en pv-infra-noc (Postgres + LocalExecutor + webserver + scheduler + cli)
#   * llaves persistentes (FERNET + WEBSERVER SECRET) en /opt/stacks/noc/airflow/env
#   * DB init + migrate forzado y restart webserver/scheduler para evitar loop "db init"
#   * salud por HTTP /health (no por docker healthcheck)
#
# Requiere:
# - Docker y compose (v2 o v1) instalados (playbook 21)
# - group_vars/trading_secrets.yml con credenciales (opcional; defaults incluidos)
# ==============================================================================

# ------------------------------------------------------------------------------
# PLAY 1: IBKR en pv-infra-master (con VNC)
# ------------------------------------------------------------------------------
- name: 36 - Desplegar contenedor IBKR en nodo master (con VNC)
  hosts: pv-infra-master
  become: true
  gather_facts: true

  vars:
    ibkr_stack_dir: "/opt/stacks/master/ibkr"
    ibkr_compose_file: "{{ ibkr_stack_dir }}/docker-compose.yml"
    ibkr_env_file: "{{ ibkr_stack_dir }}/env/ibkr.env"

    # Imagen IBKR (puedes ajustar si ya definiste otra)
    ibkr_image: "{{ ibkr_image | default('gnzsnz/ib-gateway:stable') }}"
    ibkr_container_name: "ibkr-gateway"

    # Puertos
    ibkr_vnc_port: "{{ ibkr_vnc_port | default('5900') }}"
    ibkr_gateway_port: "{{ ibkr_gateway_port | default('4002') }}"   # paper/default en muchas imágenes
    ibkr_api_port: "{{ ibkr_api_port | default('4001') }}"          # opcional

    # Credenciales / variables (idealmente en trading_secrets.yml)
    ibkr_username: "{{ ibkr_username | default('IBKR_USER') }}"
    ibkr_password: "{{ ibkr_password | default('IBKR_PASS') }}"
    ibkr_vnc_password: "{{ ibkr_vnc_password | default('ChangeMe_VNC_123!') }}"
    ibkr_trading_mode: "{{ ibkr_trading_mode | default('paper') }}" # paper/live depende imagen

  tasks:
    - name: Detectar comando compose (docker compose v2 o docker-compose v1)
      ansible.builtin.shell: |
        set -e
        if docker compose version >/dev/null 2>&1; then
          echo "docker compose"
        elif docker-compose --version >/dev/null 2>&1; then
          echo "docker-compose"
        else
          echo "none"
        fi
      args:
        executable: /bin/bash
      register: compose_detect
      changed_when: false

    - name: Set compose_cmd
      ansible.builtin.set_fact:
        compose_cmd: "{{ compose_detect.stdout }}"
      changed_when: false

    - name: Fallar si no hay compose disponible
      ansible.builtin.fail:
        msg: "No se encontró 'docker compose' (v2) ni 'docker-compose' (v1). Instala docker-compose-plugin o docker-compose."
      when: compose_cmd == "none"

    - name: Crear carpeta base para stack IBKR
      ansible.builtin.file:
        path: "{{ ibkr_stack_dir }}"
        state: directory
        owner: root
        group: root
        mode: "0755"

    - name: Crear subcarpetas de configuración y logs de IBKR
      ansible.builtin.file:
        path: "{{ item }}"
        state: directory
        owner: root
        group: root
        mode: "0755"
      loop:
        - "{{ ibkr_stack_dir }}/config"
        - "{{ ibkr_stack_dir }}/logs"
        - "{{ ibkr_stack_dir }}/env"

    - name: Crear archivo .env real para IBKR
      ansible.builtin.copy:
        dest: "{{ ibkr_env_file }}"
        owner: root
        group: root
        mode: "0600"
        content: |
          IBKR_USERNAME={{ ibkr_username }}
          IBKR_PASSWORD={{ ibkr_password }}
          IBKR_TRADING_MODE={{ ibkr_trading_mode }}
          VNC_PASSWORD={{ ibkr_vnc_password }}

    - name: Crear docker-compose para IBKR (habilita VNC 5900)
      ansible.builtin.copy:
        dest: "{{ ibkr_compose_file }}"
        owner: root
        group: root
        mode: "0644"
        content: |
          services:
            ibkr:
              image: {{ ibkr_image }}
              container_name: {{ ibkr_container_name }}
              restart: unless-stopped
              env_file:
                - {{ ibkr_env_file }}
              ports:
                - "{{ ibkr_vnc_port }}:5900"
                - "{{ ibkr_gateway_port }}:4002"
                - "{{ ibkr_api_port }}:4001"
              volumes:
                - "{{ ibkr_stack_dir }}/config:/config"
                - "{{ ibkr_stack_dir }}/logs:/var/log"
              shm_size: "512m"

    - name: Detener stack IBKR previo (si existe)
      ansible.builtin.shell: |
        set +e
        cd "{{ ibkr_stack_dir }}"
        {{ compose_cmd }} -f "{{ ibkr_compose_file }}" down --remove-orphans
        exit 0
      args:
        executable: /bin/bash
      changed_when: true

    - name: Desplegar stack IBKR con compose
      ansible.builtin.shell: |
        set -e
        cd "{{ ibkr_stack_dir }}"
        {{ compose_cmd }} -f "{{ ibkr_compose_file }}" up -d
      args:
        executable: /bin/bash
      changed_when: true


# ------------------------------------------------------------------------------
# PLAY 2: Airflow en pv-infra-noc (Postgres + LocalExecutor + webserver + scheduler + cli)
# ------------------------------------------------------------------------------
- name: 36 - Desplegar Airflow plataforma en NOC (Postgres + LocalExecutor + CLI admin estable + Keys + Admin asegurado)
  hosts: pv-infra-noc
  become: true
  gather_facts: true

  vars:
    airflow_stack_dir: "/opt/stacks/noc/airflow"
    airflow_compose_file: "{{ airflow_stack_dir }}/docker-compose.yml"
    airflow_env_dir: "{{ airflow_stack_dir }}/env"

    airflow_image: "{{ airflow_image | default('apache/airflow:2.10.2') }}"
    airflow_postgres_image: "{{ airflow_postgres_image | default('postgres:15') }}"

    airflow_port: "{{ airflow_port | default('8081') }}"

    # credenciales DB (puedes mover a trading_secrets.yml)
    airflow_db_user: "{{ airflow_db_user | default('airflow') }}"
    airflow_db_password: "{{ airflow_db_password | default('airflow') }}"
    airflow_db_name: "{{ airflow_db_name | default('airflow') }}"

    # admin web
    airflow_admin_username: "{{ airflow_admin_username | default('admin') }}"
    airflow_admin_password: "{{ airflow_admin_password | default('admin') }}"
    airflow_admin_email: "{{ airflow_admin_email | default('admin@example.com') }}"

    # performance / estabilidad
    airflow_webserver_workers: "{{ airflow_webserver_workers | default('1') }}"

    # paths
    airflow_dags_prod: "{{ airflow_stack_dir }}/dags/prod"
    airflow_dags_backtest: "{{ airflow_stack_dir }}/dags/backtest"
    airflow_logs_dir: "{{ airflow_stack_dir }}/logs"
    airflow_plugins_dir: "{{ airflow_stack_dir }}/plugins"

    # archivos de llaves persistentes
    airflow_webserver_secret_key_file: "{{ airflow_env_dir }}/airflow_webserver_secret_key.txt"
    airflow_fernet_key_file: "{{ airflow_env_dir }}/airflow_fernet_key.txt"

  tasks:
    - name: Detectar comando compose (docker compose v2 o docker-compose v1)
      ansible.builtin.shell: |
        set -e
        if docker compose version >/dev/null 2>&1; then
          echo "docker compose"
        elif docker-compose --version >/dev/null 2>&1; then
          echo "docker-compose"
        else
          echo "none"
        fi
      args:
        executable: /bin/bash
      register: compose_detect
      changed_when: false

    - name: Set compose_cmd
      ansible.builtin.set_fact:
        compose_cmd: "{{ compose_detect.stdout }}"
      changed_when: false

    - name: Fallar si no hay compose disponible
      ansible.builtin.fail:
        msg: "No se encontró 'docker compose' (v2) ni 'docker-compose' (v1). Instala docker-compose-plugin o docker-compose."
      when: compose_cmd == "none"

    - name: Crear carpeta base para stack Airflow
      ansible.builtin.file:
        path: "{{ airflow_stack_dir }}"
        state: directory
        owner: root
        group: root
        mode: "0755"

    - name: Crear carpeta env para llaves persistentes
      ansible.builtin.file:
        path: "{{ airflow_env_dir }}"
        state: directory
        owner: root
        group: root
        mode: "0755"

    - name: Crear carpetas de DAGs, logs y plugins (prod y backtest)
      ansible.builtin.file:
        path: "{{ item }}"
        state: directory
        owner: "50000"
        group: "0"
        mode: "0775"
      loop:
        - "{{ airflow_dags_prod }}"
        - "{{ airflow_dags_backtest }}"
        - "{{ airflow_logs_dir }}"
        - "{{ airflow_plugins_dir }}"

    - name: Verificar si existe AIRFLOW webserver secret key persistente
      ansible.builtin.stat:
        path: "{{ airflow_webserver_secret_key_file }}"
      register: webserver_secret_stat

    - name: Generar AIRFLOW webserver secret key (solo si no existe)
      ansible.builtin.shell: |
        set -e
        python3 - <<'PY'
        import secrets
        print(secrets.token_hex(32))
        PY
      args:
        executable: /bin/bash
      register: gen_web_secret
      when: not webserver_secret_stat.stat.exists
      changed_when: true

    - name: Guardar AIRFLOW webserver secret key en archivo (si fue generada)
      ansible.builtin.copy:
        dest: "{{ airflow_webserver_secret_key_file }}"
        owner: root
        group: root
        mode: "0600"
        content: "{{ gen_web_secret.stdout | trim }}\n"
      when: not webserver_secret_stat.stat.exists

    - name: Verificar si existe AIRFLOW fernet key persistente
      ansible.builtin.stat:
        path: "{{ airflow_fernet_key_file }}"
      register: fernet_stat

    - name: Generar AIRFLOW fernet key urlsafe (solo si no existe)
      ansible.builtin.shell: |
        set -e
        python3 - <<'PY'
        import base64, os
        # 32 bytes => Fernet key base64 urlsafe (44 chars típicamente)
        print(base64.urlsafe_b64encode(os.urandom(32)).decode())
        PY
      args:
        executable: /bin/bash
      register: gen_fernet
      when: not fernet_stat.stat.exists
      changed_when: true

    - name: Guardar AIRFLOW fernet key en archivo (si fue generada)
      ansible.builtin.copy:
        dest: "{{ airflow_fernet_key_file }}"
        owner: root
        group: root
        mode: "0600"
        content: "{{ gen_fernet.stdout | trim }}\n"
      when: not fernet_stat.stat.exists

    - name: Leer AIRFLOW webserver secret key desde archivo
      ansible.builtin.slurp:
        src: "{{ airflow_webserver_secret_key_file }}"
      register: slurp_web_secret

    - name: Leer AIRFLOW fernet key desde archivo
      ansible.builtin.slurp:
        src: "{{ airflow_fernet_key_file }}"
      register: slurp_fernet

    - name: Set facts con llaves para docker-compose
      ansible.builtin.set_fact:
        airflow_webserver_secret_key: "{{ (slurp_web_secret.content | b64decode).strip() }}"
        airflow_fernet_key: "{{ (slurp_fernet.content | b64decode).strip() }}"
      changed_when: false

    - name: Crear docker-compose de Airflow (Postgres + LocalExecutor + webserver + scheduler + cli + keys)
      ansible.builtin.copy:
        dest: "{{ airflow_compose_file }}"
        owner: root
        group: root
        mode: "0644"
        content: |
          services:
            postgres:
              image: {{ airflow_postgres_image }}
              container_name: airflow-postgres
              environment:
                POSTGRES_USER: {{ airflow_db_user }}
                POSTGRES_PASSWORD: {{ airflow_db_password }}
                POSTGRES_DB: {{ airflow_db_name }}
              volumes:
                - {{ airflow_stack_dir }}/pgdata:/var/lib/postgresql/data
              healthcheck:
                test: ["CMD-SHELL", "pg_isready -U {{ airflow_db_user }} -d {{ airflow_db_name }}"]
                interval: 5s
                timeout: 5s
                retries: 30
              restart: unless-stopped

            airflow-webserver:
              image: {{ airflow_image }}
              container_name: airflow-webserver
              depends_on:
                postgres:
                  condition: service_healthy
              environment:
                AIRFLOW__CORE__EXECUTOR: LocalExecutor
                AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://{{ airflow_db_user }}:{{ airflow_db_password }}@postgres:5432/{{ airflow_db_name }}
                AIRFLOW__CORE__FERNET_KEY: "{{ airflow_fernet_key }}"
                AIRFLOW__WEBSERVER__SECRET_KEY: "{{ airflow_webserver_secret_key }}"
                AIRFLOW__WEBSERVER__WORKERS: "{{ airflow_webserver_workers }}"
                AIRFLOW__CORE__LOAD_EXAMPLES: "False"
                AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "True"
                AIRFLOW__WEBSERVER__RBAC: "True"
                AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "True"
                AIRFLOW__CORE__ENABLE_XCOM_PICKLING: "False"
              volumes:
                - {{ airflow_dags_prod }}:/opt/airflow/dags/prod
                - {{ airflow_dags_backtest }}:/opt/airflow/dags/backtest
                - {{ airflow_logs_dir }}:/opt/airflow/logs
                - {{ airflow_plugins_dir }}:/opt/airflow/plugins
              ports:
                - "{{ airflow_port }}:8080"
              command: >
                bash -lc "airflow webserver"
              restart: unless-stopped
              shm_size: "512m"

            airflow-scheduler:
              image: {{ airflow_image }}
              container_name: airflow-scheduler
              depends_on:
                postgres:
                  condition: service_healthy
              environment:
                AIRFLOW__CORE__EXECUTOR: LocalExecutor
                AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://{{ airflow_db_user }}:{{ airflow_db_password }}@postgres:5432/{{ airflow_db_name }}
                AIRFLOW__CORE__FERNET_KEY: "{{ airflow_fernet_key }}"
                AIRFLOW__WEBSERVER__SECRET_KEY: "{{ airflow_webserver_secret_key }}"
                AIRFLOW__CORE__LOAD_EXAMPLES: "False"
              volumes:
                - {{ airflow_dags_prod }}:/opt/airflow/dags/prod
                - {{ airflow_dags_backtest }}:/opt/airflow/dags/backtest
                - {{ airflow_logs_dir }}:/opt/airflow/logs
                - {{ airflow_plugins_dir }}:/opt/airflow/plugins
              command: >
                bash -lc "airflow scheduler"
              restart: unless-stopped
              shm_size: "512m"

            airflow-cli:
              image: {{ airflow_image }}
              container_name: airflow-cli
              depends_on:
                postgres:
                  condition: service_healthy
              environment:
                AIRFLOW__CORE__EXECUTOR: LocalExecutor
                AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://{{ airflow_db_user }}:{{ airflow_db_password }}@postgres:5432/{{ airflow_db_name }}
                AIRFLOW__CORE__FERNET_KEY: "{{ airflow_fernet_key }}"
                AIRFLOW__WEBSERVER__SECRET_KEY: "{{ airflow_webserver_secret_key }}"
              volumes:
                - {{ airflow_dags_prod }}:/opt/airflow/dags/prod
                - {{ airflow_dags_backtest }}:/opt/airflow/dags/backtest
                - {{ airflow_logs_dir }}:/opt/airflow/logs
                - {{ airflow_plugins_dir }}:/opt/airflow/plugins
              entrypoint: /bin/bash
              command: -lc "sleep infinity"
              restart: unless-stopped

    - name: Eliminar contenedor legacy "airflow" si existe (evita choques)
      ansible.builtin.shell: |
        set +e
        docker rm -f airflow >/dev/null 2>&1 || true
        exit 0
      args:
        executable: /bin/bash
      changed_when: false

    - name: Detener stack Airflow previo (si existe) y limpiar orphans
      ansible.builtin.shell: |
        set +e
        cd "{{ airflow_stack_dir }}"
        {{ compose_cmd }} -f "{{ airflow_compose_file }}" down --remove-orphans
        exit 0
      args:
        executable: /bin/bash
      changed_when: true

    - name: Desplegar stack Airflow con compose
      ansible.builtin.shell: |
        set -e
        cd "{{ airflow_stack_dir }}"
        {{ compose_cmd }} -f "{{ airflow_compose_file }}" up -d
      args:
        executable: /bin/bash
      changed_when: true

    - name: Esperar a Postgres listo (healthy)
      ansible.builtin.shell: |
        set -e
        docker inspect airflow-postgres --format '{{"{{"}}.State.Health.Status{{"}}"}}' | grep -q healthy
      args:
        executable: /bin/bash
      register: pg_health
      retries: 60
      delay: 5
      until: pg_health.rc == 0
      changed_when: false

    - name: Esperar que airflow-cli esté running (para comandos administrativos)
      ansible.builtin.shell: |
        set -e
        docker inspect airflow-cli --format '{{"{{"}}.State.Status{{"}}"}}' | grep -q running
      args:
        executable: /bin/bash
      register: cli_running
      retries: 60
      delay: 2
      until: cli_running.rc == 0
      changed_when: false

    # --------------------------------------------------------------------------
    # FIX PRINCIPAL: DB init + migrate explícito + restart webserver/scheduler
    # --------------------------------------------------------------------------
    - name: Inicializar DB Airflow (idempotente) via airflow-cli
      ansible.builtin.command:
        cmd: docker exec airflow-cli airflow db init
      register: db_init
      retries: 10
      delay: 6
      until: db_init.rc == 0
      changed_when: false

    - name: Ejecutar migraciones Airflow (idempotente) via airflow-cli
      ansible.builtin.command:
        cmd: docker exec airflow-cli airflow db migrate
      register: db_migrate
      retries: 10
      delay: 6
      until: db_migrate.rc == 0
      changed_when: false

    - name: Listar usuarios Airflow via airflow-cli
      ansible.builtin.command:
        cmd: docker exec airflow-cli airflow users list
      register: airflow_users_list
      changed_when: false
      failed_when: false

    - name: Crear usuario admin si no existe (via airflow-cli)
      ansible.builtin.command:
        cmd: >
          docker exec airflow-cli airflow users create
          --username {{ airflow_admin_username }}
          --password {{ airflow_admin_password }}
          --firstname Admin
          --lastname User
          --role Admin
          --email {{ airflow_admin_email }}
      register: admin_create
      when: >
        ('No data found' in (airflow_users_list.stdout | default('')))
        or (airflow_admin_username not in (airflow_users_list.stdout | default('')))
      retries: 5
      delay: 5
      until: admin_create.rc == 0
      changed_when: true
      failed_when: false

    - name: Forzar password del admin (idempotente) via airflow-cli
      ansible.builtin.shell: |
        set -e
        docker exec airflow-cli airflow users set-password \
          --username "{{ airflow_admin_username }}" \
          --password "{{ airflow_admin_password }}"
      args:
        executable: /bin/bash
      register: admin_set_pw
      retries: 5
      delay: 5
      until: admin_set_pw.rc == 0
      changed_when: true

    - name: Reiniciar webserver y scheduler para tomar DB lista
      ansible.builtin.command:
        cmd: docker restart airflow-webserver airflow-scheduler
      changed_when: true

    - name: Esperar que el endpoint /health responda (HTTP real)
      ansible.builtin.shell: |
        set -e
        curl -fsS http://localhost:{{ airflow_port }}/health >/dev/null
      args:
        executable: /bin/bash
      register: http_health
      retries: 90
      delay: 2
      until: http_health.rc == 0
      changed_when: false

    - name: Mostrar URL de Airflow
      ansible.builtin.debug:
        msg: "Airflow debería estar disponible en: http://{{ ansible_host | default(inventory_hostname) }}:{{ airflow_port }} (user={{ airflow_admin_username }})"
      changed_when: false

# ------------------------------------------------------------------------------
# PLAY 3 (opcional / placeholder): Kafka + Spark
# Nota: No despliega Kafka/Spark aquí porque no indicaste aún en qué hosts
#       ni si usas Confluent, Bitnami, o standalone. Lo dejamos como base.
# ------------------------------------------------------------------------------
- name: 36 - Placeholder (no-op) Kafka/Spark (pendiente definir hosts)
  hosts: localhost
  gather_facts: false
  tasks:
    - name: Info
      ansible.builtin.debug:
        msg: >
          Kafka/Spark no se despliegan en este 36. Define dónde (pv-infra-soc o pv-infra-master)
          y el stack (KRaft/Confluent) y lo integramos con Airflow (DAG -> produce a Kafka -> Spark consume).