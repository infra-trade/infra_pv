---
########################################################################
# 36 - IBKR en MASTER (VNC)
#    + Airflow "plataforma" en NOC (Postgres + LocalExecutor + CLI admin estable + Keys persistentes + Admin asegurado)
#    + Kafka (KRaft) + Spark + Kafka UI + Observabilidad en SOC
#
# Requisitos:
# - Docker + docker-compose v1 o docker compose v2 instalado (playbook 21 ya lo hace)
# - trading_secrets.yml (vault) con credenciales necesarias
########################################################################

#######################################################################
# PLAY 1: IBKR en MASTER (VNC)
#######################################################################
- name: 36 - Desplegar contenedor IBKR en nodo master (con VNC)
  hosts: pv-infra-master
  become: true

  vars_files:
    - "../inventories/infra/group_vars/trading_secrets.yml"

  vars:
    ibkr_stack_base: /opt/stacks/master/ibkr
    ibkr_compose_file: "{{ ibkr_stack_base }}/docker-compose.yml"

  tasks:
    - name: Detectar comando compose (docker compose v2 o docker-compose v1)
      ansible.builtin.shell: |
        set -e
        if docker compose version >/dev/null 2>&1; then
          echo "docker compose"
        elif docker-compose --version >/dev/null 2>&1; then
          echo "docker-compose"
        else
          echo "none"
        fi
      args:
        executable: /bin/bash
      register: compose_detect
      changed_when: false

    - name: Set compose_cmd
      ansible.builtin.set_fact:
        compose_cmd: "{{ compose_detect.stdout | trim }}"

    - name: Fallar si no hay compose disponible
      ansible.builtin.fail:
        msg: "No se encontró docker compose v2 ni docker-compose v1 en {{ inventory_hostname }}. Ejecuta primero el playbook 21."
      when: compose_cmd == "none"

    - name: Crear carpeta base para stack IBKR
      ansible.builtin.file:
        path: "{{ ibkr_stack_base }}"
        state: directory
        owner: root
        group: root
        mode: "0755"

    - name: Crear subcarpetas de configuración y logs de IBKR
      ansible.builtin.file:
        path: "{{ item }}"
        state: directory
        owner: root
        group: root
        mode: "0755"
      loop:
        - "{{ ibkr_stack_base }}/config"
        - "{{ ibkr_stack_base }}/logs"
        - "{{ ibkr_stack_base }}/env"

    - name: Crear archivo .env real para IBKR desde plantilla
      ansible.builtin.template:
        src: "ibkr.env.j2"
        dest: "{{ ibkr_stack_base }}/env/ibkr.env"
        owner: root
        group: root
        mode: "0600"

    - name: Crear docker-compose para IBKR (habilita VNC 5900)
      ansible.builtin.copy:
        dest: "{{ ibkr_compose_file }}"
        owner: root
        group: root
        mode: "0644"
        content: |
          version: "3.8"

          services:
            ibkr-gateway:
              image: ghcr.io/gnzsnz/ib-gateway:latest
              container_name: ibkr-gateway
              restart: unless-stopped
              env_file:
                - ./env/ibkr.env
              ports:
                - "4001:4001"
                - "5900:5900"
              volumes:
                - ./logs:/opt/ibgateway/logs
                - ./config:/opt/ibgateway/config
              networks:
                - ibkr_net

          networks:
            ibkr_net:
              driver: bridge

    - name: Detener stack IBKR previo (si existe)
      ansible.builtin.command:
        cmd: "{{ compose_cmd }} -f {{ ibkr_compose_file }} down"
      args:
        chdir: "{{ ibkr_stack_base }}"
      register: ibkr_down_result
      failed_when: false
      changed_when: ibkr_down_result.rc == 0

    - name: Desplegar stack IBKR con compose
      ansible.builtin.command:
        cmd: "{{ compose_cmd }} -f {{ ibkr_compose_file }} up -d"
      args:
        chdir: "{{ ibkr_stack_base }}"

#######################################################################
# PLAY 2: AIRFLOW en NOC (DB init + CLI admin + healthchecks)
#######################################################################
- name: 36 - Desplegar Airflow plataforma en NOC (Postgres + LocalExecutor + CLI admin estable + Keys + Admin asegurado)
  hosts: pv-infra-noc
  become: true

  vars_files:
    - "../inventories/infra/group_vars/trading_secrets.yml"

  vars:
    airflow_base_dir: /opt/stacks/noc/airflow
    airflow_compose_file: "{{ airflow_base_dir }}/docker-compose.yml"
    airflow_uid: "50000"

    # DB creds (ideal: en Vault dentro de trading_secrets.yml)
    airflow_db_user: "{{ airflow_postgres_user | default('airflow') }}"
    airflow_db_password: "{{ airflow_postgres_password | default('airflow') }}"
    airflow_db_name: "{{ airflow_postgres_db | default('airflow') }}"

    # Admin (desde vault idealmente)
    airflow_admin_username: "{{ airflow_admin_username | default('admin') }}"
    airflow_admin_password: "{{ airflow_admin_password | default('admin') }}"
    airflow_admin_email: "{{ airflow_admin_email | default('admin@example.com') }}"

    # Keys persistentes (para que NO cambien entre redeploys)
    airflow_env_dir: "{{ airflow_base_dir }}/env"
    airflow_secret_key_file: "{{ airflow_env_dir }}/airflow_webserver_secret_key.txt"
    airflow_fernet_key_file: "{{ airflow_env_dir }}/airflow_fernet_key.txt"

    # Webserver workers (no templating recursivo)
    airflow_webserver_workers: 1

  tasks:
    - name: Detectar comando compose (docker compose v2 o docker-compose v1)
      ansible.builtin.shell: |
        set -e
        if docker compose version >/dev/null 2>&1; then
          echo "docker compose"
        elif docker-compose --version >/dev/null 2>&1; then
          echo "docker-compose"
        else
          echo "none"
        fi
      args:
        executable: /bin/bash
      register: compose_detect
      changed_when: false

    - name: Set compose_cmd
      ansible.builtin.set_fact:
        compose_cmd: "{{ compose_detect.stdout | trim }}"

    - name: Fallar si no hay compose disponible
      ansible.builtin.fail:
        msg: "No se encontró docker compose v2 ni docker-compose v1 en {{ inventory_hostname }}. Ejecuta primero el playbook 21."
      when: compose_cmd == "none"

    - name: Crear carpeta base para stack Airflow
      ansible.builtin.file:
        path: "{{ airflow_base_dir }}"
        state: directory
        owner: "{{ airflow_uid }}"
        group: "{{ airflow_uid }}"
        mode: "0775"

    - name: Crear carpeta env para llaves persistentes
      ansible.builtin.file:
        path: "{{ airflow_env_dir }}"
        state: directory
        owner: "{{ airflow_uid }}"
        group: "{{ airflow_uid }}"
        mode: "0770"

    - name: Crear carpetas de DAGs, logs y plugins (prod y backtest)
      ansible.builtin.file:
        path: "{{ item }}"
        state: directory
        owner: "{{ airflow_uid }}"
        group: "{{ airflow_uid }}"
        mode: "0775"
      loop:
        - "{{ airflow_base_dir }}/dags/prod"
        - "{{ airflow_base_dir }}/dags/backtest"
        - "{{ airflow_base_dir }}/logs"
        - "{{ airflow_base_dir }}/plugins"

    ####################################################################
    # Keys persistentes: SECRET_KEY y FERNET_KEY
    ####################################################################
    - name: Verificar si existe AIRFLOW webserver secret key persistente
      ansible.builtin.stat:
        path: "{{ airflow_secret_key_file }}"
      register: secret_key_stat

    - name: Generar AIRFLOW webserver secret key (solo si no existe)
      ansible.builtin.shell: |
        set -euo pipefail
        umask 077
        openssl rand -hex 32 > "{{ airflow_secret_key_file }}"
        chown {{ airflow_uid }}:{{ airflow_uid }} "{{ airflow_secret_key_file }}"
        chmod 0600 "{{ airflow_secret_key_file }}"
      args:
        executable: /bin/bash
      when: not secret_key_stat.stat.exists

    - name: Verificar si existe AIRFLOW fernet key persistente
      ansible.builtin.stat:
        path: "{{ airflow_fernet_key_file }}"
      register: fernet_key_stat

    - name: Generar AIRFLOW fernet key urlsafe (solo si no existe)
      ansible.builtin.shell: |
        set -euo pipefail
        umask 077
        python3 - <<'PY' > "{{ airflow_fernet_key_file }}"
        import os, base64
        print(base64.urlsafe_b64encode(os.urandom(32)).decode())
        PY
        chown {{ airflow_uid }}:{{ airflow_uid }} "{{ airflow_fernet_key_file }}"
        chmod 0600 "{{ airflow_fernet_key_file }}"
      args:
        executable: /bin/bash
      when: not fernet_key_stat.stat.exists

    - name: Leer AIRFLOW webserver secret key desde archivo
      ansible.builtin.slurp:
        src: "{{ airflow_secret_key_file }}"
      register: secret_key_slurp

    - name: Leer AIRFLOW fernet key desde archivo
      ansible.builtin.slurp:
        src: "{{ airflow_fernet_key_file }}"
      register: fernet_key_slurp

    - name: Set facts con llaves para docker-compose
      ansible.builtin.set_fact:
        airflow_webserver_secret_key: "{{ (secret_key_slurp.content | b64decode).strip() }}"
        airflow_fernet_key: "{{ (fernet_key_slurp.content | b64decode).strip() }}"

    ####################################################################
    # Compose Airflow (incluye airflow-cli siempre arriba para admin)
    ####################################################################
    - name: Crear docker-compose de Airflow (Postgres + LocalExecutor + webserver + scheduler + cli + keys)
      ansible.builtin.copy:
        dest: "{{ airflow_compose_file }}"
        owner: "{{ airflow_uid }}"
        group: "{{ airflow_uid }}"
        mode: "0644"
        content: |
          version: "3.8"

          x-airflow-common: &airflow-common
            image: apache/airflow:2.10.2
            restart: unless-stopped
            environment:
              - AIRFLOW__CORE__EXECUTOR=LocalExecutor
              - AIRFLOW__CORE__LOAD_EXAMPLES=False
              - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=True
              - AIRFLOW_UID={{ airflow_uid }}
              - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://{{ airflow_db_user }}:{{ airflow_db_password }}@postgres:5432/{{ airflow_db_name }}

              # Keys persistentes
              - AIRFLOW__WEBSERVER__SECRET_KEY={{ airflow_webserver_secret_key }}
              - AIRFLOW__CORE__FERNET_KEY={{ airflow_fernet_key }}

            volumes:
              - ./dags/prod:/opt/airflow/dags/prod
              - ./dags/backtest:/opt/airflow/dags/backtest
              - ./logs:/opt/airflow/logs
              - ./plugins:/opt/airflow/plugins
            depends_on:
              - postgres

          services:
            postgres:
              image: postgres:15
              container_name: airflow-postgres
              restart: unless-stopped
              environment:
                - POSTGRES_USER={{ airflow_db_user }}
                - POSTGRES_PASSWORD={{ airflow_db_password }}
                - POSTGRES_DB={{ airflow_db_name }}
              volumes:
                - postgres_data:/var/lib/postgresql/data
              healthcheck:
                test: ["CMD-SHELL", "pg_isready -U {{ airflow_db_user }} -d {{ airflow_db_name }}"]
                interval: 10s
                timeout: 5s
                retries: 10

            # Contenedor estable para comandos administrativos (NO depende del webserver)
            airflow-cli:
              <<: *airflow-common
              container_name: airflow-cli
              entrypoint: /bin/bash
              command: -lc "sleep infinity"
              restart: unless-stopped

            airflow-webserver:
              <<: *airflow-common
              container_name: airflow-webserver
              ports:
                - "8081:8080"
              environment:
                - AIRFLOW__WEBSERVER__WORKERS={{ airflow_webserver_workers }}
              command: webserver
              healthcheck:
                test: ["CMD-SHELL", "curl -fsS http://localhost:8080/health || exit 1"]
                interval: 10s
                timeout: 5s
                retries: 30

            airflow-scheduler:
              <<: *airflow-common
              container_name: airflow-scheduler
              command: scheduler

          volumes:
            postgres_data:

          networks:
            default:
              name: airflow_noc_net

    - name: Eliminar contenedor legacy "airflow" si existe (evita choques)
      ansible.builtin.command:
        cmd: docker rm -f airflow
      failed_when: false
      changed_when: false

    - name: Detener stack Airflow previo (si existe) y limpiar orphans
      ansible.builtin.command:
        cmd: "{{ compose_cmd }} -f {{ airflow_compose_file }} down --remove-orphans"
      args:
        chdir: "{{ airflow_base_dir }}"
      register: airflow_down_result
      failed_when: false
      changed_when: airflow_down_result.rc == 0

    - name: Desplegar stack Airflow con compose
      ansible.builtin.command:
        cmd: "{{ compose_cmd }} -f {{ airflow_compose_file }} up -d"
      args:
        chdir: "{{ airflow_base_dir }}"

    - name: Esperar a Postgres listo (healthy)
      ansible.builtin.command:
        cmd: docker inspect -f '{{ "{{" }}.State.Health.Status{{ "}}" }}' airflow-postgres
      register: pg_health
      retries: 60
      delay: 3
      until: pg_health.stdout.strip() == "healthy"
      changed_when: false

    - name: Esperar que airflow-cli esté running (para comandos administrativos)
      ansible.builtin.command:
        cmd: docker inspect -f '{{ "{{" }}.State.Status{{ "}}" }}' airflow-cli
      register: cli_state
      retries: 60
      delay: 2
      until: cli_state.stdout.strip() == "running"
      changed_when: false

    ####################################################################
    # Init DB y admin via airflow-cli (estable, evita restarts del webserver)
    ####################################################################
    - name: Ejecutar migraciones Airflow (idempotente) via airflow-cli
      ansible.builtin.command:
        cmd: docker exec airflow-cli airflow db migrate
      register: db_migrate
      retries: 10
      delay: 6
      until: db_migrate.rc == 0
      changed_when: db_migrate.rc == 0

    - name: Listar usuarios Airflow via airflow-cli
      ansible.builtin.command:
        cmd: docker exec airflow-cli airflow users list
      register: airflow_users_list
      changed_when: false
      failed_when: false

    - name: Crear usuario admin si no existe (via airflow-cli)
      ansible.builtin.shell: >
        docker exec airflow-cli bash -lc
        "airflow users create
        --username {{ airflow_admin_username }}
        --password {{ airflow_admin_password }}
        --firstname Admin
        --lastname User
        --role Admin
        --email {{ airflow_admin_email }}"
      register: airflow_admin_create
      when: >
        ('No data found' in (airflow_users_list.stdout | default('')))
        or (airflow_admin_username not in (airflow_users_list.stdout | default('')))
      retries: 10
      delay: 6
      until: airflow_admin_create.rc == 0
      failed_when: airflow_admin_create.rc != 0
      changed_when: airflow_admin_create.rc == 0

    - name: Forzar password del admin (idempotente) via airflow-cli
      ansible.builtin.shell: >
        docker exec airflow-cli bash -lc
        "airflow users reset-password
        --username {{ airflow_admin_username }}
        --password {{ airflow_admin_password }}"
      register: airflow_reset_pwd
      retries: 5
      delay: 4
      until: airflow_reset_pwd.rc == 0
      failed_when: false
      changed_when: airflow_reset_pwd.rc == 0

    - name: Esperar que airflow-webserver esté healthy (healthcheck docker)
      ansible.builtin.command:
        cmd: docker inspect -f '{{ "{{" }}.State.Health.Status{{ "}}" }}' airflow-webserver
      register: ws_health
      retries: 60
      delay: 3
      until: ws_health.stdout.strip() == "healthy"
      changed_when: false
      failed_when: false

    - name: Si airflow-webserver no llega a healthy, mostrar logs y fallar
      ansible.builtin.shell: |
        set -e
        echo "==== docker ps (airflow) ===="
        docker ps --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}" | egrep "airflow-webserver|airflow-scheduler|airflow-postgres|airflow-cli" || true
        echo ""
        echo "==== logs airflow-webserver (tail 250) ===="
        docker logs --tail 250 airflow-webserver || true
        echo ""
        echo "==== logs airflow-scheduler (tail 250) ===="
        docker logs --tail 250 airflow-scheduler || true
        echo ""
        echo "==== inspect airflow-webserver ===="
        docker inspect airflow-webserver --format 'Status={{.State.Status}} RestartCount={{.RestartCount}} OOMKilled={{.State.OOMKilled}} ExitCode={{.State.ExitCode}} Health={{.State.Health.Status}}' || true
        exit 1
      args:
        executable: /bin/bash
      when: ws_health.stdout.strip() != "healthy"

    - name: Mostrar URLs útiles de Airflow
      ansible.builtin.debug:
        msg:
          - "Airflow UI => http://{{ hostvars[inventory_hostname].ansible_host | default(inventory_hostname) }}:8081"
          - "Admin user => {{ airflow_admin_username }} / (password desde trading_secrets.yml)"

#######################################################################
# PLAY 3: SOC - Kafka (KRaft) + Spark + Kafka UI + Prometheus + Grafana
#######################################################################
- name: 36 - Desplegar Kafka (KRaft) + Spark + Kafka UI + Observabilidad (SOC)
  hosts: pv-infra-soc
  become: true

  vars:
    streaming_base_dir: /opt/stacks/soc/streaming
    streaming_compose_file: "{{ streaming_base_dir }}/docker-compose.yml"

    # IMPORTANTE: soc host real
    soc_host_ip: "192.168.5.87"

    # Tu cluster_id KRaft ya generado
    kafka_cluster_id: "ZHeJ4mbEQqO8SSZ2fEXbFQ"

  tasks:
    - name: Detectar comando compose (docker compose v2 o docker-compose v1)
      ansible.builtin.shell: |
        set -e
        if docker compose version >/dev/null 2>&1; then
          echo "docker compose"
        elif docker-compose --version >/dev/null 2>&1; then
          echo "docker-compose"
        else
          echo "none"
        fi
      args:
        executable: /bin/bash
      register: compose_detect
      changed_when: false

    - name: Set compose_cmd
      ansible.builtin.set_fact:
        compose_cmd: "{{ compose_detect.stdout | trim }}"

    - name: Fallar si no hay compose disponible
      ansible.builtin.fail:
        msg: "No se encontró docker compose v2 ni docker-compose v1 en {{ inventory_hostname }}. Ejecuta primero el playbook 21."
      when: compose_cmd == "none"

    - name: Crear carpeta base para stack de streaming / cómputo
      ansible.builtin.file:
        path: "{{ streaming_base_dir }}"
        state: directory
        owner: root
        group: root
        mode: "0755"

    - name: Crear carpetas para Kafka, Spark y observabilidad
      ansible.builtin.file:
        path: "{{ item }}"
        state: directory
        owner: root
        group: root
        mode: "0755"
      loop:
        - "{{ streaming_base_dir }}/kafka/data"
        - "{{ streaming_base_dir }}/kafka/config"
        - "{{ streaming_base_dir }}/spark/jobs/prod"
        - "{{ streaming_base_dir }}/spark/jobs/backtest"
        - "{{ streaming_base_dir }}/spark/config"
        - "{{ streaming_base_dir }}/monitoring/prometheus"
        - "{{ streaming_base_dir }}/monitoring/grafana/provisioning/datasources"
        - "{{ streaming_base_dir }}/monitoring/grafana/provisioning/dashboards"
        - "{{ streaming_base_dir }}/monitoring/grafana/dashboards"

    - name: Crear prometheus.yml
      ansible.builtin.copy:
        dest: "{{ streaming_base_dir }}/monitoring/prometheus/prometheus.yml"
        owner: root
        group: root
        mode: "0644"
        content: |
          global:
            scrape_interval: 15s
          scrape_configs:
            - job_name: "kafka_exporter"
              static_configs:
                - targets: ["kafka-exporter:9308"]

    - name: Crear datasource Prometheus para Grafana
      ansible.builtin.copy:
        dest: "{{ streaming_base_dir }}/monitoring/grafana/provisioning/datasources/datasource.yml"
        owner: root
        group: root
        mode: "0644"
        content: |
          apiVersion: 1
          datasources:
            - name: Prometheus
              type: prometheus
              access: proxy
              url: http://prometheus:9090
              isDefault: true

    - name: Crear provider de dashboards para Grafana
      ansible.builtin.copy:
        dest: "{{ streaming_base_dir }}/monitoring/grafana/provisioning/dashboards/provider.yml"
        owner: root
        group: root
        mode: "0644"
        content: |
          apiVersion: 1
          providers:
            - name: "Trading Dashboards"
              orgId: 1
              folder: "Trading"
              type: file
              disableDeletion: false
              editable: true
              options:
                path: /var/lib/grafana/dashboards

    - name: Crear docker-compose para Kafka (KRaft) + Spark + Kafka UI + Observabilidad
      ansible.builtin.copy:
        dest: "{{ streaming_compose_file }}"
        owner: root
        group: root
        mode: "0644"
        content: |
          version: "3.8"

          services:
            kafka:
              image: confluentinc/cp-kafka:latest
              container_name: kafka
              restart: unless-stopped
              ports:
                - "9092:9092"
              environment:
                KAFKA_PROCESS_ROLES: "broker,controller"
                KAFKA_NODE_ID: 1
                KAFKA_CLUSTER_ID: "{{ kafka_cluster_id }}"

                KAFKA_CONTROLLER_QUORUM_VOTERS: "1@kafka:9093"
                KAFKA_CONTROLLER_LISTENER_NAMES: "CONTROLLER"

                KAFKA_LISTENERS: "PLAINTEXT://0.0.0.0:29092,PLAINTEXT_HOST://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093"
                KAFKA_ADVERTISED_LISTENERS: "PLAINTEXT://kafka:29092,PLAINTEXT_HOST://{{ soc_host_ip }}:9092"
                KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: "PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT,CONTROLLER:PLAINTEXT"
                KAFKA_INTER_BROKER_LISTENER_NAME: "PLAINTEXT"

                KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
                KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
                KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
                KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
                KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
                KAFKA_LOG_DIRS: "/var/lib/kafka/data"
              volumes:
                - ./kafka/data:/var/lib/kafka/data
                - ./kafka/config:/etc/kafka

            kafka-ui:
              image: provectuslabs/kafka-ui:latest
              container_name: kafka-ui
              restart: unless-stopped
              depends_on:
                - kafka
              ports:
                - "8084:8080"
              environment:
                - KAFKA_CLUSTERS_0_NAME=SOC-KAFKA
                - KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=kafka:29092
                - DYNAMIC_CONFIG_ENABLED=true

            kafka-exporter:
              image: danielqsj/kafka-exporter:latest
              container_name: kafka-exporter
              restart: unless-stopped
              depends_on:
                - kafka
              command:
                - --kafka.server=kafka:29092
              ports:
                - "9308:9308"

            spark-master:
              image: bde2020/spark-master:3.3.0-hadoop3.3
              container_name: spark-master
              restart: unless-stopped
              ports:
                - "8080:8080"
                - "7077:7077"
              environment:
                - INIT_DAEMON_STEP=setup_spark

            spark-worker-1:
              image: bde2020/spark-worker:3.3.0-hadoop3.3
              container_name: spark-worker-1
              restart: unless-stopped
              depends_on:
                - spark-master
              ports:
                - "8082:8081"
              environment:
                - SPARK_MASTER=spark://spark-master:7077

            spark-worker-2:
              image: bde2020/spark-worker:3.3.0-hadoop3.3
              container_name: spark-worker-2
              restart: unless-stopped
              depends_on:
                - spark-master
              ports:
                - "8083:8081"
              environment:
                - SPARK_MASTER=spark://spark-master:7077

            spark-submit:
              image: bde2020/spark-submit:3.3.0-hadoop3.3
              container_name: spark-submit
              restart: unless-stopped
              depends_on:
                - spark-master
              volumes:
                - ./spark/jobs/prod:/opt/spark/jobs/prod
                - ./spark/jobs/backtest:/opt/spark/jobs/backtest
                - ./spark/config:/opt/spark/config
              environment:
                - SPARK_MASTER_URL=spark://spark-master:7077
              command: [ "sleep", "infinity" ]

            prometheus:
              image: prom/prometheus:latest
              container_name: prometheus
              restart: unless-stopped
              ports:
                - "9090:9090"
              volumes:
                - ./monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
              depends_on:
                - kafka-exporter

            grafana:
              image: grafana/grafana-oss:latest
              container_name: grafana
              restart: unless-stopped
              ports:
                - "3000:3000"
              environment:
                - GF_SECURITY_ADMIN_USER=admin
                - GF_SECURITY_ADMIN_PASSWORD=admin
              volumes:
                - ./monitoring/grafana/provisioning:/etc/grafana/provisioning
                - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards
              depends_on:
                - prometheus

          networks:
            default:
              name: streaming_default

    - name: Detener stack Streaming previo (si existe)
      ansible.builtin.command:
        cmd: "{{ compose_cmd }} -f {{ streaming_compose_file }} down"
      args:
        chdir: "{{ streaming_base_dir }}"
      register: streaming_down_result
      failed_when: false
      changed_when: streaming_down_result.rc == 0

    - name: Desplegar stack Streaming con compose
      ansible.builtin.command:
        cmd: "{{ compose_cmd }} -f {{ streaming_compose_file }} up -d"
      args:
        chdir: "{{ streaming_base_dir }}"

    - name: Esperar a que Kafka responda (check básico)
      ansible.builtin.shell: |
        set -e
        docker exec kafka bash -lc "kafka-topics --bootstrap-server kafka:29092 --list >/dev/null"
      args:
        executable: /bin/bash
      register: kafka_ready
      retries: 40
      delay: 3
      until: kafka_ready.rc == 0
      changed_when: false

    - name: Mostrar URLs útiles del stack SOC
      ansible.builtin.debug:
        msg:
          - "Kafka UI => http://{{ soc_host_ip }}:8084"
          - "Spark Master UI => http://{{ soc_host_ip }}:8080"
          - "Grafana => http://{{ soc_host_ip }}:3000 (admin/admin)"
          - "Prometheus => http://{{ soc_host_ip }}:9090"