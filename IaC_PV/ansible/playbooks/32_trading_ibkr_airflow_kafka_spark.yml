---
# ansible/playbooks/32_trading_ibkr_airflow_kafka_spark.yml
#
# Infra PV:
# - pv-infra-master: IBKR Gateway
# - pv-infra-noc: Airflow (single container) + init db + admin user
# - pv-infra-soc: Kafka + Zookeeper + Spark (bitnami/spark:3.5.4)
#
# Requisitos:
# - Playbook 21 ya instaló docker + docker-compose (v1) en los nodos.
# - Secrets en: ansible/inventories/infra/group_vars/trading_secrets.yml (vault)
# - Template IBKR: /home/pvinfra/infra_pv/IaC_PV/ansible/ansible/templates/ibkr.env.j2
#
# Nota importante:
# - En tus hosts, "docker compose" v2 NO está disponible (rc=1) y sí "docker-compose" (rc=0).
#   Por eso este playbook usa docker-compose siempre.

- name: "32 - Desplegar contenedor IBKR en nodo master"
  hosts: pv-infra-master
  become: true
  gather_facts: true

  vars_files:
    - "../inventories/infra/group_vars/trading_secrets.yml"

  vars:
    ibkr_stack_base: "/opt/stacks/master/ibkr"
    ibkr_image: "ghcr.io/gnzsnz/ib-gateway:latest"
    ibkr_container_name: "ibkr-gateway"
    ibkr_compose_file: "{{ ibkr_stack_base }}/docker-compose.yml"

  tasks:
    - name: Crear carpeta base para stack IBKR
      ansible.builtin.file:
        path: "{{ ibkr_stack_base }}"
        state: directory
        owner: root
        group: root
        mode: "0755"

    - name: Crear subcarpetas de configuración y logs de IBKR
      ansible.builtin.file:
        path: "{{ item }}"
        state: directory
        owner: root
        group: root
        mode: "0755"
      loop:
        - "{{ ibkr_stack_base }}/config"
        - "{{ ibkr_stack_base }}/logs"
        - "{{ ibkr_stack_base }}/env"

    # OJO: tu template está en /ansible/ansible/templates (según tu salida)
    - name: Crear archivo .env real para IBKR desde plantilla
      ansible.builtin.template:
        src: "/home/pvinfra/infra_pv/IaC_PV/ansible/ansible/templates/ibkr.env.j2"
        dest: "{{ ibkr_stack_base }}/env/ibkr.env"
        owner: root
        group: root
        mode: "0600"

    - name: Crear docker-compose para IBKR
      ansible.builtin.copy:
        dest: "{{ ibkr_compose_file }}"
        owner: root
        group: root
        mode: "0644"
        content: |
          version: "3.8"
          services:
            {{ ibkr_container_name }}:
              image: {{ ibkr_image }}
              container_name: {{ ibkr_container_name }}
              restart: unless-stopped
              env_file:
                - ./env/ibkr.env
              ports:
                - "4001:4001"   # TWS/IB Gateway API (paper/live depende config)
                - "4002:4002"   # opcional
                - "5900:5900"   # VNC si la imagen lo expone
              volumes:
                - ./config:/config
                - ./logs:/logs
              networks:
                - ibkr_net
          networks:
            ibkr_net:
              driver: bridge

    - name: Detener stack IBKR previo (si existe)
      ansible.builtin.command:
        cmd: docker-compose -f "{{ ibkr_compose_file }}" down
      args:
        chdir: "{{ ibkr_stack_base }}"
      register: ibkr_down
      failed_when: false
      changed_when: ibkr_down.rc == 0

    - name: Desplegar stack IBKR con docker-compose
      ansible.builtin.command:
        cmd: docker-compose -f "{{ ibkr_compose_file }}" up -d
      args:
        chdir: "{{ ibkr_stack_base }}"
      register: ibkr_up
      changed_when: ibkr_up.rc == 0


- name: "32 - Desplegar Airflow para orquestación de estrategias (NOC)"
  hosts: pv-infra-noc
  become: true
  gather_facts: true

  vars_files:
    - "../inventories/infra/group_vars/trading_secrets.yml"

  vars:
    airflow_base_dir: "/opt/stacks/noc/airflow"
    airflow_compose_file: "{{ airflow_base_dir }}/docker-compose.yml"
    airflow_container_name: "airflow"
    airflow_image: "apache/airflow:2.10.2"

    # Airflow en Docker suele usar 50000:0 o 50000:50000.
    # Tus evidencias muestran owner 50000 y group 50000 en /opt/stacks/noc/airflow.
    airflow_uid: "50000"
    airflow_gid: "50000"

    # Puerto publicado en el host (ajústalo si quieres 8080)
    airflow_host_port: "8081"

  tasks:
    - name: Crear carpeta base para stack Airflow
      ansible.builtin.file:
        path: "{{ airflow_base_dir }}"
        state: directory
        owner: "{{ airflow_uid }}"
        group: "{{ airflow_gid }}"
        mode: "0775"

    - name: Crear carpetas de DAGs, logs, plugins y env (prod y backtest)
      ansible.builtin.file:
        path: "{{ item }}"
        state: directory
        owner: "{{ airflow_uid }}"
        group: "{{ airflow_gid }}"
        mode: "0775"
      loop:
        - "{{ airflow_base_dir }}/dags/prod"
        - "{{ airflow_base_dir }}/dags/backtest"
        - "{{ airflow_base_dir }}/logs"
        - "{{ airflow_base_dir }}/plugins"
        - "{{ airflow_base_dir }}/env"

    # >>> FIX CLAVE: Fernet válido y persistente <<<
    # Genera un Fernet key correcto y lo deja en env/airflow.env con 0600.
    # Requiere python3 + cryptography en el HOST. Si no existe, verás error y habrá que instalarlo.
    - name: Generar AIRFLOW__CORE__FERNET_KEY si no existe (archivo env/airflow.env)
      ansible.builtin.shell: |
        set -e
        ENV_FILE="{{ airflow_base_dir }}/env/airflow.env"
        if [ ! -f "$ENV_FILE" ]; then
          FERNET=$(python3 - <<'PY'
from cryptography.fernet import Fernet
print(Fernet.generate_key().decode())
PY
)
          cat > "$ENV_FILE" <<EOF
AIRFLOW__CORE__FERNET_KEY=${FERNET}
EOF
          chmod 600 "$ENV_FILE"
          chown {{ airflow_uid }}:{{ airflow_gid }} "$ENV_FILE"
        fi
      args:
        executable: /bin/bash

    - name: Crear docker-compose de Airflow (single container + SQLite) con env_file (fernet)
      ansible.builtin.copy:
        dest: "{{ airflow_compose_file }}"
        owner: "{{ airflow_uid }}"
        group: "{{ airflow_gid }}"
        mode: "0644"
        content: |
          version: "3.8"
          services:
            airflow:
              image: {{ airflow_image }}
              container_name: {{ airflow_container_name }}
              restart: unless-stopped
              user: "{{ airflow_uid }}:{{ airflow_gid }}"
              env_file:
                - ./env/airflow.env
              environment:
                AIRFLOW__CORE__EXECUTOR: "SequentialExecutor"
                AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: "sqlite:////opt/airflow/airflow.db"
                AIRFLOW__CORE__LOAD_EXAMPLES: "False"
                AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "True"
                # Si quieres setear timezone:
                AIRFLOW__CORE__DEFAULT_TIMEZONE: "America/Costa_Rica"
              volumes:
                - ./dags:/opt/airflow/dags
                - ./logs:/opt/airflow/logs
                - ./plugins:/opt/airflow/plugins
              ports:
                - "{{ airflow_host_port }}:8080"
              networks:
                - airflow_noc_net
          networks:
            airflow_noc_net:
              driver: bridge

    - name: Detener stack Airflow previo (si existe)
      ansible.builtin.command:
        cmd: docker-compose -f "{{ airflow_compose_file }}" down
      args:
        chdir: "{{ airflow_base_dir }}"
      register: airflow_down
      failed_when: false
      changed_when: airflow_down.rc == 0

    - name: Desplegar stack Airflow con docker-compose
      ansible.builtin.command:
        cmd: docker-compose -f "{{ airflow_compose_file }}" up -d
      args:
        chdir: "{{ airflow_base_dir }}"
      register: airflow_up
      changed_when: airflow_up.rc == 0

    - name: Esperar a que el contenedor Airflow esté "Up"
      ansible.builtin.shell: |
        set -e
        docker ps --format '{{"{{"}}.Names{{"}}"}} {{ "{{"{{"}}.Status{{"}}"}}' | grep -E '^{{ airflow_container_name }}\s+Up'
      register: airflow_running
      retries: 10
      delay: 3
      until: airflow_running.rc == 0
      changed_when: false

    # Airflow 2.10 recomienda migrate en vez de init
    - name: Migrar DB de Airflow (airflow db migrate)
      ansible.builtin.command:
        cmd: docker exec "{{ airflow_container_name }}" airflow db migrate
      register: airflow_db_migrate
      retries: 10
      delay: 6
      until: airflow_db_migrate.rc == 0
      changed_when: true

    # Crear admin desde secrets (si no está definido, cae a defaults seguros)
    - name: Definir credenciales admin de Airflow con defaults si no vienen en secrets
      ansible.builtin.set_fact:
        airflow_admin_username_eff: "{{ airflow_admin_username | default('admin') }}"
        airflow_admin_password_eff: "{{ airflow_admin_password | default('admin') }}"
        airflow_admin_email_eff: "{{ airflow_admin_email | default('admin@example.com') }}"
        airflow_admin_firstname_eff: "{{ airflow_admin_firstname | default('Admin') }}"
        airflow_admin_lastname_eff: "{{ airflow_admin_lastname | default('User') }}"

    - name: Crear usuario admin si no existe (idempotente)
      ansible.builtin.shell: |
        set -e
        if docker exec "{{ airflow_container_name }}" airflow users list | awk '{print $2}' | grep -qx "{{ airflow_admin_username_eff }}"; then
          echo "User exists"
        else
          docker exec "{{ airflow_container_name }}" airflow users create \
            --username "{{ airflow_admin_username_eff }}" \
            --password "{{ airflow_admin_password_eff }}" \
            --firstname "{{ airflow_admin_firstname_eff }}" \
            --lastname "{{ airflow_admin_lastname_eff }}" \
            --role "Admin" \
            --email "{{ airflow_admin_email_eff }}"
        fi
      args:
        executable: /bin/bash
      register: airflow_admin_create
      changed_when: "'User exists' not in airflow_admin_create.stdout"

    - name: Mostrar URL de Airflow
      ansible.builtin.debug:
        msg:
          - "Airflow UI: http://{{ ansible_host }}:{{ airflow_host_port }}"
          - "Usuario: {{ airflow_admin_username_eff }}"
          - "Password: (definida en vault trading_secrets.yml o default 'admin')"


- name: "32 - Desplegar Kafka + Zookeeper + Spark (SOC)"
  hosts: pv-infra-soc
  become: true
  gather_facts: true

  vars:
    streaming_base_dir: "/opt/stacks/soc/streaming"
    streaming_compose_file: "{{ streaming_base_dir }}/docker-compose.yml"

    kafka_image: "confluentinc/cp-kafka:latest"
    zk_image: "confluentinc/cp-zookeeper:latest"

    # FIX: bitnami/spark:latest no existe; usa un tag existente
    spark_image: "bitnami/spark:3.5.4"

  tasks:
    - name: Crear carpeta base para stack de streaming / cómputo
      ansible.builtin.file:
        path: "{{ streaming_base_dir }}"
        state: directory
        owner: root
        group: root
        mode: "0755"

    - name: Crear carpetas para Kafka y Spark (incluye prod/backtest)
      ansible.builtin.file:
        path: "{{ item }}"
        state: directory
        owner: root
        group: root
        mode: "0755"
      loop:
        - "{{ streaming_base_dir }}/kafka/data"
        - "{{ streaming_base_dir }}/kafka/config"
        - "{{ streaming_base_dir }}/spark/jobs/prod"
        - "{{ streaming_base_dir }}/spark/jobs/backtest"
        - "{{ streaming_base_dir }}/spark/config"

    - name: Crear docker-compose para Kafka + Zookeeper + Spark
      ansible.builtin.copy:
        dest: "{{ streaming_compose_file }}"
        owner: root
        group: root
        mode: "0644"
        content: |
          version: "3.8"
          services:
            zookeeper:
              image: {{ zk_image }}
              container_name: zookeeper
              restart: unless-stopped
              environment:
                ZOOKEEPER_CLIENT_PORT: 2181
                ZOOKEEPER_TICK_TIME: 2000
              ports:
                - "2181:2181"

            kafka:
              image: {{ kafka_image }}
              container_name: kafka
              restart: unless-stopped
              depends_on:
                - zookeeper
              ports:
                - "9092:9092"
              environment:
                KAFKA_BROKER_ID: 1
                KAFKA_ZOOKEEPER_CONNECT: "zookeeper:2181"
                KAFKA_LISTENERS: "PLAINTEXT://0.0.0.0:9092"
                KAFKA_ADVERTISED_LISTENERS: "PLAINTEXT://{{ ansible_host }}:9092"
                KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
              volumes:
                - ./kafka/data:/var/lib/kafka/data

            spark-master:
              image: {{ spark_image }}
              container_name: spark-master
              restart: unless-stopped
              environment:
                - SPARK_MODE=master
              ports:
                - "7077:7077"
                - "8080:8080"
              volumes:
                - ./spark/jobs:/opt/spark/jobs

            spark-worker:
              image: {{ spark_image }}
              container_name: spark-worker
              restart: unless-stopped
              depends_on:
                - spark-master
              environment:
                - SPARK_MODE=worker
                - SPARK_MASTER_URL=spark://spark-master:7077
              ports:
                - "8082:8081"
              volumes:
                - ./spark/jobs:/opt/spark/jobs

    - name: Detener stack Kafka + Spark previo (si existe)
      ansible.builtin.command:
        cmd: docker-compose -f "{{ streaming_compose_file }}" down
      args:
        chdir: "{{ streaming_base_dir }}"
      register: streaming_down
      failed_when: false
      changed_when: streaming_down.rc == 0

    - name: Desplegar stack Kafka + Spark con docker-compose
      ansible.builtin.command:
        cmd: docker-compose -f "{{ streaming_compose_file }}" up -d
      args:
        chdir: "{{ streaming_base_dir }}"
      register: streaming_up
      changed_when: streaming_up.rc == 0

    - name: Mostrar endpoints SOC
      ansible.builtin.debug:
        msg:
          - "Kafka: {{ ansible_host }}:9092"
          - "Zookeeper: {{ ansible_host }}:2181"
          - "Spark Master UI: http://{{ ansible_host }}:8080"
          - "Spark Worker UI: http://{{ ansible_host }}:8082"