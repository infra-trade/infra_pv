---
########################################################################
# 32 - IBKR en MASTER, Airflow en NOC, Kafka + Spark en SOC
########################################################################

- name: 32 - Desplegar contenedor IBKR en nodo master
  hosts: pv-infra-master
  become: true

  # Cargamos secretos cifrados (IBKR, etc.)
  vars_files:
    - "../inventories/infra/group_vars/trading_secrets.yml"

  vars:
    ibkr_stack_base: /opt/stacks/master/ibkr
    ibkr_compose_file: "{{ ibkr_stack_base }}/docker-compose.yml"

  tasks:
    - name: Crear carpeta base para stack IBKR
      ansible.builtin.file:
        path: "{{ ibkr_stack_base }}"
        state: directory
        owner: root
        group: root
        mode: "0755"

    - name: Crear subcarpetas de configuración y logs de IBKR
      ansible.builtin.file:
        path: "{{ item }}"
        state: directory
        owner: root
        group: root
        mode: "0755"
      loop:
        - "{{ ibkr_stack_base }}/config"
        - "{{ ibkr_stack_base }}/logs"
        - "{{ ibkr_stack_base }}/env"

    - name: Crear archivo .env real para IBKR desde plantilla
      ansible.builtin.template:
        src: "../ansible/templates/ibkr.env.j2"
        dest: "{{ ibkr_stack_base }}/env/ibkr.env"
        owner: root
        group: root
        mode: "0600"

    - name: Crear docker-compose para IBKR
      ansible.builtin.copy:
        dest: "{{ ibkr_compose_file }}"
        owner: root
        group: root
        mode: "0644"
        content: |
          version: "3.8"

          services:
            ibkr-gateway:
              image: ghcr.io/gnzsnz/ib-gateway:latest
              container_name: ibkr-gateway
              restart: unless-stopped
              env_file:
                - ./env/ibkr.env
              volumes:
                - ./logs:/opt/ibgateway/logs
                - ./config:/opt/ibgateway/config
              ports:
                - "4001:4001"
              networks:
                - ibkr_net

          networks:
            ibkr_net:
              driver: bridge

    - name: Detener stack IBKR previo (si existe)
      ansible.builtin.command:
        cmd: docker-compose -f "{{ ibkr_compose_file }}" down
      args:
        chdir: "{{ ibkr_stack_base }}"
      register: ibkr_down_result
      failed_when: false
      changed_when: ibkr_down_result.rc == 0

    - name: Desplegar stack IBKR con docker-compose
      ansible.builtin.command:
        cmd: docker-compose -f "{{ ibkr_compose_file }}" up -d
      args:
        chdir: "{{ ibkr_stack_base }}"

########################################################################

- name: 32 - Desplegar Airflow para orquestación de estrategias (NOC)
  hosts: pv-infra-noc
  become: true

  vars:
    airflow_base_dir: /opt/stacks/noc/airflow
    airflow_compose_file: "{{ airflow_base_dir }}/docker-compose.yml"

  tasks:
    - name: Crear carpeta base para stack Airflow
      ansible.builtin.file:
        path: "{{ airflow_base_dir }}"
        state: directory
        owner: "50000"
        group: "50000"
        mode: "0775"

    - name: Crear carpetas de DAGs, logs y plugins (prod y backtest)
      ansible.builtin.file:
        path: "{{ item }}"
        state: directory
        owner: "50000"
        group: "50000"
        mode: "0775"
      loop:
        - "{{ airflow_base_dir }}/dags/prod"
        - "{{ airflow_base_dir }}/dags/backtest"
        - "{{ airflow_base_dir }}/logs"
        - "{{ airflow_base_dir }}/plugins"

    - name: Crear docker-compose de Airflow (modo simple / SequentialExecutor)
      ansible.builtin.copy:
        dest: "{{ airflow_compose_file }}"
        owner: "50000"
        group: "50000"
        mode: "0644"
        content: |
          version: "3.8"

          services:
            airflow:
              image: apache/airflow:2.10.2
              container_name: airflow
              restart: unless-stopped
              environment:
                - AIRFLOW__CORE__EXECUTOR=SequentialExecutor
                - AIRFLOW__CORE__FERNET_KEY=please_change_me
                - AIRFLOW__CORE__LOAD_EXAMPLES=False
                - AIRFLOW__WEBSERVER__RBAC=True
                - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=True
                - AIRFLOW_UID=50000
              volumes:
                - ./dags/prod:/opt/airflow/dags/prod
                - ./dags/backtest:/opt/airflow/dags/backtest
                - ./logs:/opt/airflow/logs
                - ./plugins:/opt/airflow/plugins
              ports:
                - "8081:8080"
              command: >
                bash -c "
                airflow db upgrade &&
                airflow users create
                  --username admin
                  --password admin
                  --firstname Admin
                  --lastname User
                  --role Admin
                  --email admin@example.com || true &&
                airflow webserver
                "

          networks:
            default:
              name: airflow_noc_net

    - name: Detener stack Airflow previo (si existe)
      ansible.builtin.command:
        cmd: docker-compose -f "{{ airflow_compose_file }}" down
      args:
        chdir: "{{ airflow_base_dir }}"
      register: airflow_down_result
      failed_when: false
      changed_when: airflow_down_result.rc == 0

    - name: Desplegar stack Airflow con docker-compose
      ansible.builtin.command:
        cmd: docker-compose -f "{{ airflow_compose_file }}" up -d
      args:
        chdir: "{{ airflow_base_dir }}"

########################################################################

- name: 32 - Desplegar Kafka + Zookeeper + Spark (SOC)
  hosts: pv-infra-soc
  become: true

  vars:
    streaming_base_dir: /opt/stacks/soc/streaming
    streaming_compose_file: "{{ streaming_base_dir }}/docker-compose.yml"

  tasks:
    - name: Crear carpeta base para stack de streaming / cómputo
      ansible.builtin.file:
        path: "{{ streaming_base_dir }}"
        state: directory
        owner: root
        group: root
        mode: "0755"

    - name: Crear carpetas para Kafka y Spark (incluye prod/backtest)
      ansible.builtin.file:
        path: "{{ item }}"
        state: directory
        owner: root
        group: root
        mode: "0755"
      loop:
        - "{{ streaming_base_dir }}/kafka/data"
        - "{{ streaming_base_dir }}/kafka/config"
        - "{{ streaming_base_dir }}/spark/jobs/prod"
        - "{{ streaming_base_dir }}/spark/jobs/backtest"
        - "{{ streaming_base_dir }}/spark/config"

    - name: Crear docker-compose para Kafka + Zookeeper + Spark
      ansible.builtin.copy:
        dest: "{{ streaming_compose_file }}"
        owner: root
        group: root
        mode: "0644"
        content: |
          version: "3.8"

          services:
            zookeeper:
              image: confluentinc/cp-zookeeper:latest
              container_name: zookeeper
              environment:
                ZOOKEEPER_CLIENT_PORT: 2181
                ZOOKEEPER_TICK_TIME: 2000

            kafka:
              image: confluentinc/cp-kafka:latest
              container_name: kafka
              depends_on:
                - zookeeper
              ports:
                - "9092:9092"
              environment:
                KAFKA_BROKER_ID: 1
                KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
                KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
                KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
                KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
                KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
                KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
                KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
                KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
              volumes:
                - ./kafka/data:/var/lib/kafka/data
                - ./kafka/config:/etc/kafka

            spark-master:
              image: bde2020/spark-master:3.3.0-hadoop3.3
              container_name: spark-master
              ports:
                - "8080:8080"
                - "7077:7077"
              environment:
                - INIT_DAEMON_STEP=setup_spark

            spark-worker-1:
              image: bde2020/spark-worker:3.3.0-hadoop3.3
              container_name: spark-worker-1
              depends_on:
                - spark-master
              ports:
                - "8082:8081"
              environment:
                - SPARK_MASTER=spark://spark-master:7077

            spark-worker-2:
              image: bde2020/spark-worker:3.3.0-hadoop3.3
              container_name: spark-worker-2
              depends_on:
                - spark-master
              ports:
                - "8083:8081"
              environment:
                - SPARK_MASTER=spark://spark-master:7077

            spark-submit:
              image: bde2020/spark-submit:3.3.0-hadoop3.3
              container_name: spark-submit
              depends_on:
                - spark-master
              volumes:
                - ./spark/jobs/prod:/opt/spark/jobs/prod
                - ./spark/jobs/backtest:/opt/spark/jobs/backtest
                - ./spark/config:/opt/spark/config
              environment:
                - SPARK_MASTER_URL=spark://spark-master:7077
              command: [ "sleep", "infinity" ]

          networks:
            default:
              name: streaming_default

    - name: Detener stack Kafka + Spark previo (si existe)
      ansible.builtin.command:
        cmd: docker-compose -f "{{ streaming_compose_file }}" down
      args:
        chdir: "{{ streaming_base_dir }}"
      register: streaming_down_result
      failed_when: false
      changed_when: streaming_down_result.rc == 0

    - name: Desplegar stack Kafka + Spark con docker-compose
      ansible.builtin.command:
        cmd: docker-compose -f "{{ streaming_compose_file }}" up -d
      args:
        chdir: "{{ streaming_base_dir }}"